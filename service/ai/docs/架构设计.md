# 架构设计

## 分层架构

Galay AI Service 采用三层架构：

```
┌─────────────────────────────────────┐
│           API Layer (api/)          │  ← HTTP 接口、中间件、路由
├─────────────────────────────────────┤
│        Service Layer (services/)    │  ← 业务逻辑、对话管理、RAG
├─────────────────────────────────────┤
│          Core Layer (core/)         │  ← 向量存储、文档加载、嵌入
├─────────────────────────────────────┤
│     Infrastructure (utils/, config) │  ← 日志、异常、配置
└─────────────────────────────────────┘
```

## 模块职责

### API Layer (`src/api/`)
- `router.py` — 路由注册中心，聚合所有子路由
- `chat.py` — 聊天接口（同步 + SSE 流式），接收用户消息，调用 ChatService
- `search.py` — 搜索接口，直接查询向量存储
- `admin.py` — 管理接口（重建索引、统计、清除会话）
- `middleware.py` — 全局异常处理、请求日志、CORS、请求限流（slowapi）

### Service Layer (`src/services/`)
- `chat_service.py` — 对话服务，管理会话记忆（OrderedDict LRU），协调 RAG 流程，支持流式输出
- `rag_service.py` — RAG 核心，检索文档 + 调用 LLM 生成回答（同步/流式）
- `index_service.py` — 索引管理，构建/重建/统计

### Core Layer (`src/core/`)
- `vector_store.py` — ChromaDB 向量存储的初始化、查询、写入
- `embeddings.py` — OpenAI Embedding 模型的懒加载管理
- `document_loader.py` — Markdown 文档的递归加载和元数据提取
- `text_splitter.py` — 针对 Markdown 优化的文本分割

### Infrastructure
- `config.py` — Pydantic Settings，自动从 `.env` 加载配置
- `utils/logger.py` — 结构化日志（开发环境彩色输出，生产环境 JSON）
- `utils/exceptions.py` — 自定义异常体系，映射到 HTTP 状态码

## 数据流

### 聊天请求

```
HTTP POST /api/chat (同步) 或 /api/chat/stream (SSE 流式)
       │
       ▼
  ChatService.chat() / chat_stream()
       │
       ├─ RAGService.retrieve() → 检索 Top-K 文档
       │
       ├─ _build_messages() → 构建消息列表 (system + context + history + question)
       │
       ├─ LLM.invoke() / LLM.astream() → 生成回答
       │
       ├─ _append_history() → 保存对话到 OrderedDict（LRU 淘汰，最多 100 会话）
       │
       └─ 提取去重的引用来源
       │
       ▼
  ChatResponse / SSE events { content, done, sources }
```

### 索引构建

```
build_index.py / POST /api/rebuild
       │
       ▼
  DocumentLoader.load_all()
       │
       ├─ 遍历 7 个 Galay 项目目录
       ├─ 递归查找 *.md 文件
       ├─ 跳过 .git, build, node_modules 等
       └─ 分割为 chunks（1000 字符，200 重叠）
       │
       ▼
  VectorStore.from_documents()
       │
       ├─ EmbeddingManager.get_embeddings() → OpenAI API
       └─ ChromaDB 持久化存储
```

## 生命周期管理

应用使用 FastAPI 的 `lifespan` 上下文管理器：

1. 启动时：校验配置 → 初始化向量存储 → 创建服务实例
2. 运行中：处理请求，服务实例作为全局单例
3. 关闭时：日志记录，资源自动释放
