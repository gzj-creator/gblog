from collections import OrderedDict
import re
from typing import Any, AsyncGenerator, Dict, List

from langchain_core.messages import AIMessage, HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI

from src.config import settings
from src.core.markdown_blocks import markdown_to_blocks
from src.core.markdown_normalizer import normalize_markdown_content
from src.core.vector_store import VectorStoreManager
from src.services.rag_service import RAGService, SYSTEM_PROMPT
from src.utils.exceptions import ChatServiceError
from src.utils.logger import get_logger

logger = get_logger(__name__)

MAX_SESSIONS = 100
MAX_HISTORY_ROUNDS = 20
STREAM_EMIT_MIN_CHARS = 16
STREAM_EMIT_MAX_CHARS = 120

_FORBIDDEN_SCHEDULER_APIS = (
    re.compile(r"\b(?:IoContext|IOContext)\s*::\s*GetInstance\s*\(\s*\)", re.IGNORECASE),
    re.compile(r"\bioContext\b"),
    re.compile(r"\bIoContext\b"),
)


class ChatService:
    """å¯¹è¯æœåŠ¡ï¼ˆå«ä¼šè¯è®°å¿†ï¼‰"""

    def __init__(self, vector_store: VectorStoreManager):
        self._vector_store = vector_store
        self._rag = RAGService(vector_store)
        self._llm = ChatOpenAI(
            model=settings.MODEL_NAME,
            temperature=settings.TEMPERATURE,
            openai_api_key=settings.OPENAI_API_KEY,
            openai_api_base=settings.OPENAI_API_BASE,
        )
        # session_id -> List[{"role": "user"|"assistant", "content": str}]
        self._histories: OrderedDict[str, List[dict]] = OrderedDict()

    # ------------------------------------------------------------------
    # å…¬å¼€æ¥å£
    # ------------------------------------------------------------------
    def chat(self, message: str, session_id: str = "default") -> Dict[str, Any]:
        """å¸¦ä¼šè¯è®°å¿†çš„å¯¹è¯"""
        try:
            docs = self._rag.retrieve(message, k=4)
            sources = _extract_sources(docs)
            messages = self._build_messages(message, docs, session_id)

            response = self._llm.invoke(messages)
            answer = _normalize_answer_text(_extract_message_text(response))
            if not answer.strip():
                answer = "æŠ±æ­‰ï¼Œæ¨¡å‹è¿”å›äº†ç©ºå†…å®¹ï¼Œè¯·ç¨åé‡è¯•ã€‚"
            blocks = _build_answer_blocks(answer)

            self._append_history(session_id, message, answer)

            return {
                "success": True,
                "response": answer,
                "blocks": blocks,
                "sources": sources,
                "session_id": session_id,
            }
        except Exception as e:
            logger.error(f"Chat error: {e}")
            raise ChatServiceError(f"Chat failed: {e}")

    async def chat_stream(
        self, message: str, session_id: str = "default"
    ) -> AsyncGenerator[dict, None]:
        """å¸¦ä¼šè¯è®°å¿†çš„æµå¼å¯¹è¯"""
        try:
            docs = self._rag.retrieve(message, k=4)
            sources = _extract_sources(docs)
            messages = self._build_messages(message, docs, session_id)

            raw_answer_parts: List[str] = []
            streamed_parts: List[str] = []
            stream_buffer = ""
            emitted_any = False

            async for chunk in self._llm.astream(messages):
                text = _extract_message_text(chunk)
                if text:
                    raw_answer_parts.append(text)
                    stream_buffer += _sanitize_stream_fragment(text)
                    while True:
                        emit_piece, stream_buffer = _pop_stream_emit_piece(stream_buffer)
                        if not emit_piece:
                            break
                        emitted_any = True
                        streamed_parts.append(emit_piece)
                        partial_text, partial_blocks = _build_stream_preview(streamed_parts)
                        if partial_text:
                            yield {"replace": partial_text, "blocks": partial_blocks, "partial": True}
                        else:
                            yield {"content": emit_piece}

            raw_answer = "".join(raw_answer_parts)
            if not raw_answer.strip():
                # éƒ¨åˆ† OpenAI å…¼å®¹å®ç°å¯èƒ½åœ¨ stream ä¸­ä¸ç»™ contentï¼Œå…œåº•ä¸€æ¬¡åŒæ­¥è°ƒç”¨ã€‚
                fallback = self._llm.invoke(messages)
                raw_answer = _extract_message_text(fallback).strip()
                normalized_answer = _normalize_answer_text(raw_answer)
                if not normalized_answer:
                    normalized_answer = "æŠ±æ­‰ï¼Œæ¨¡å‹è¿”å›äº†ç©ºå†…å®¹ï¼Œè¯·ç¨åé‡è¯•ã€‚"
                answer_blocks = _build_answer_blocks(normalized_answer)
                yield {"replace": normalized_answer, "blocks": answer_blocks}
                answer = normalized_answer
            else:
                tail_piece = _finalize_stream_tail(stream_buffer)
                if tail_piece:
                    emitted_any = True
                    streamed_parts.append(tail_piece)
                    partial_text, partial_blocks = _build_stream_preview(streamed_parts)
                    if partial_text:
                        yield {"replace": partial_text, "blocks": partial_blocks, "partial": True}
                    else:
                        yield {"content": tail_piece}

                normalized_answer = _normalize_answer_text(raw_answer)
                if not normalized_answer:
                    normalized_answer = "æŠ±æ­‰ï¼Œæ¨¡å‹è¿”å›äº†ç©ºå†…å®¹ï¼Œè¯·ç¨åé‡è¯•ã€‚"
                answer_blocks = _build_answer_blocks(normalized_answer)

                # é˜²æ­¢æ¸…æ´—åæ— å¯æ˜¾ç¤ºå†…å®¹æ—¶æµä¸ºç©ºï¼Œå…œåº•è¡¥å‘æ ‡å‡†åˆ†å—æ–‡æœ¬ã€‚
                if not emitted_any:
                    yield {"replace": normalized_answer, "blocks": answer_blocks}
                else:
                    # ç»Ÿä¸€è§„åˆ™æœ€ç»ˆè½åœ°ï¼šå§‹ç»ˆä»¥è§„èŒƒåŒ–åçš„å…¨æ–‡è¦†ç›–æµå¼ä¸­é—´æ€ã€‚
                    yield {"replace": normalized_answer, "blocks": answer_blocks}
                answer = normalized_answer

            self._append_history(session_id, message, answer)

            yield {"done": True, "sources": sources, "blocks": answer_blocks}
        except Exception as e:
            logger.error(f"Chat stream error: {e}")
            yield {"error": str(e)}

    def query(self, message: str) -> Dict[str, Any]:
        """æ— è®°å¿†çš„å•æ¬¡é—®ç­”"""
        try:
            docs = self._rag.retrieve(message, k=4)
            if not docs:
                return {
                    "success": True,
                    "response": "æŠ±æ­‰ï¼Œæˆ‘åœ¨æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•æ¢ä¸ªæ–¹å¼æé—®ã€‚",
                    "blocks": _build_answer_blocks("æŠ±æ­‰ï¼Œæˆ‘åœ¨æ–‡æ¡£ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚è¯·å°è¯•æ¢ä¸ªæ–¹å¼æé—®ã€‚"),
                    "sources": [],
                }
            answer = _normalize_answer_text(self._rag.generate(message, docs))
            blocks = _build_answer_blocks(answer)
            sources = _extract_sources(docs)
            return {"success": True, "response": answer, "blocks": blocks, "sources": sources}
        except Exception as e:
            logger.error(f"Query error: {e}")
            raise ChatServiceError(f"Query failed: {e}")

    def clear_session(self, session_id: str) -> None:
        if session_id in self._histories:
            del self._histories[session_id]
            logger.info(f"Cleared memory for session: {session_id}")

    def get_active_sessions(self) -> List[str]:
        return list(self._histories.keys())

    # ------------------------------------------------------------------
    # å†…éƒ¨
    # ------------------------------------------------------------------
    def _build_messages(self, message: str, docs: list, session_id: str) -> list:
        """æ„å»º LLM æ¶ˆæ¯åˆ—è¡¨ï¼šsystem + context + history + user"""
        context = "\n\n".join(doc.page_content for doc in docs)

        system_content = f"""{SYSTEM_PROMPT}

è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

{context}"""

        messages = [SystemMessage(content=system_content)]

        # è¿½åŠ å†å²å¯¹è¯
        history = self._histories.get(session_id, [])
        for entry in history:
            if entry["role"] == "user":
                messages.append(HumanMessage(content=entry["content"]))
            else:
                messages.append(AIMessage(content=entry["content"]))

        messages.append(HumanMessage(content=message))
        return messages

    def _append_history(self, session_id: str, user_msg: str, assistant_msg: str) -> None:
        """è¿½åŠ å¯¹è¯è®°å½•ï¼Œç»´æŠ¤ LRU æ·˜æ±°å’Œè½®æ•°é™åˆ¶"""
        if session_id in self._histories:
            # ç§»åˆ°æœ«å°¾ï¼ˆLRUï¼‰
            self._histories.move_to_end(session_id)
        else:
            # æ·˜æ±°æœ€æ—§çš„ session
            if len(self._histories) >= MAX_SESSIONS:
                evicted = next(iter(self._histories))
                del self._histories[evicted]
                logger.info(f"Evicted oldest session: {evicted}")
            self._histories[session_id] = []

        history = self._histories[session_id]
        history.append({"role": "user", "content": user_msg})
        history.append({"role": "assistant", "content": assistant_msg})

        # ä¿ç•™æœ€è¿‘ N è½®ï¼ˆæ¯è½® 2 æ¡ï¼‰
        max_entries = MAX_HISTORY_ROUNDS * 2
        if len(history) > max_entries:
            self._histories[session_id] = history[-max_entries:]


def _extract_sources(documents: list) -> List[Dict[str, str]]:
    """æå–å»é‡åçš„æºæ–‡æ¡£ä¿¡æ¯"""
    sources: List[Dict[str, str]] = []
    seen: set = set()
    for doc in documents:
        meta = doc.metadata
        key = f"{meta.get('project', 'unknown')}:{meta.get('source', 'unknown')}"
        if key not in seen:
            sources.append({
                "project": meta.get("project", "unknown"),
                "file": meta.get("source", "unknown"),
                "file_name": meta.get("file_name", "unknown"),
            })
            seen.add(key)
    return sources


def _extract_message_text(message: Any) -> str:
    """å…¼å®¹ä¸åŒ OpenAI å…¼å®¹æœåŠ¡çš„æ¶ˆæ¯ç»“æ„ï¼Œå°½å¯èƒ½æå–æ–‡æœ¬å†…å®¹ã€‚"""
    if message is None:
        return ""

    content = getattr(message, "content", message)
    additional = getattr(message, "additional_kwargs", {}) or {}
    return (
        _coerce_text(content)
        or _coerce_text(additional.get("content"))
        or _coerce_text(additional.get("reasoning_content"))
        or ""
    )


def _coerce_text(value: Any) -> str:
    if value is None:
        return ""

    if isinstance(value, str):
        return value

    if isinstance(value, list):
        parts: List[str] = []
        for item in value:
            piece = _coerce_text(item)
            if piece:
                parts.append(piece)
        return "".join(parts)

    if isinstance(value, dict):
        for key in ("text", "content", "reasoning_content"):
            piece = _coerce_text(value.get(key))
            if piece:
                return piece
        return ""

    return str(value)


def _normalize_answer_text(raw: str) -> str:
    """ç»Ÿä¸€è§„èŒƒæ¨¡å‹è¾“å‡ºï¼Œä¿è¯ä¸å…¥åº“æ–‡æ¡£ä¸€è‡´çš„ markdown è§„åˆ™ã€‚"""
    if not raw:
        return ""

    normalized = normalize_markdown_content(str(raw), target="answer", strip_decorative=True)
    return _enforce_scheduler_api_consistency(normalized)


def _enforce_scheduler_api_consistency(text: str) -> str:
    if not text:
        return ""

    if not any(pattern.search(text) for pattern in _FORBIDDEN_SCHEDULER_APIS):
        return text

    fixed = text
    fixed = re.sub(
        r"\b(?:IoContext|IOContext)\s*::\s*GetInstance\s*\(\s*\)",
        "runtime.getNextIOScheduler()",
        fixed,
        flags=re.IGNORECASE,
    )
    fixed = re.sub(r"\bioContext\b", "ioScheduler", fixed)
    fixed = re.sub(r"\bIoContext\b", "IOScheduler", fixed)

    note = (
        "è¯´æ˜ï¼šGalay å½“å‰æ²¡æœ‰ `IoContext` å•ä¾‹ APIï¼Œè¯·ä½¿ç”¨ `Runtime` è·å–è°ƒåº¦å™¨ï¼š"
        "`runtime.getNextIOScheduler()` / `runtime.getNextComputeScheduler()`ã€‚"
    )
    if note not in fixed:
        fixed = f"{fixed.rstrip()}\n\n{note}"

    logger.warning("Detected forbidden IoContext API in model output, auto-corrected")
    return fixed


def _build_answer_blocks(text: str) -> List[Dict[str, Any]]:
    if not text:
        return []
    try:
        return markdown_to_blocks(text)
    except Exception as exc:
        logger.warning(f"build markdown blocks failed: {exc}")
        return [{"type": "paragraph", "text": text}]


def _build_stream_preview(streamed_parts: List[str]) -> tuple[str, List[Dict[str, Any]]]:
    if not streamed_parts:
        return "", []
    preview_raw = "".join(streamed_parts).strip()
    if not preview_raw:
        return "", []
    preview_text = _normalize_answer_text(preview_raw)
    if not preview_text:
        return "", []
    return preview_text, _build_answer_blocks(preview_text)


def _strip_decorative_symbols(text: str) -> str:
    # å»æ‰å¸¸è§è£…é¥° emoji / å›¾æ ‡ç¬¦å·ï¼Œä¿ç•™ä¸­æ–‡æ ‡ç‚¹ä¸ Markdown åŸºç¡€ç»“æ„ã€‚
    decorative = (
        "[âœ…â˜‘âœ”âœ³âœ´â˜…â˜†â­ğŸ”¥ğŸŒŸâœ¨ğŸ’¡ğŸ”§âš™ğŸ› ğŸ“ˆğŸ“ŒğŸ“ğŸš€ğŸ¯â–¶â–ºâ– â–¡â–ªâ–«â—†â—‡â€¢Â·]"
    )
    stripped = re.sub(decorative, "", text)
    stripped = re.sub(r"\s*---+\s*", "\n", stripped)
    return stripped


def _sanitize_stream_fragment(fragment: str) -> str:
    if not fragment:
        return ""

    text = str(fragment).replace("\r\n", "\n").replace("\r", "\n")
    text = _strip_decorative_symbols(text)
    text = re.sub(r"[ \t]+", " ", text)
    return text


def _pop_stream_emit_piece(buffer: str) -> tuple[str, str]:
    if not buffer:
        return "", ""

    # ä¼˜å…ˆåœ¨å¥å­è¾¹ç•Œè¾“å‡ºï¼Œå¢å¼ºâ€œé€æ­¥å‡ºç°â€çš„ä½“æ„Ÿã€‚
    for idx, ch in enumerate(buffer):
        if idx + 1 < STREAM_EMIT_MIN_CHARS:
            continue
        if ch in "\nã€‚ï¼ï¼Ÿ!?ï¼›;":
            emit = buffer[: idx + 1].strip()
            rest = buffer[idx + 1 :]
            return emit, rest

    # å¤ªé•¿åˆ™å¼ºåˆ¶åˆ‡ä¸€æ®µï¼Œé¿å…å‰ç«¯é•¿æ—¶é—´æ— æ›´æ–°ã€‚
    if len(buffer) >= STREAM_EMIT_MAX_CHARS:
        cut = buffer.rfind(" ", 0, STREAM_EMIT_MAX_CHARS)
        if cut <= 0:
            cut = STREAM_EMIT_MAX_CHARS
        emit = buffer[:cut].strip()
        rest = buffer[cut:]
        return emit, rest

    return "", buffer


def _finalize_stream_tail(buffer: str) -> str:
    if not buffer:
        return ""
    return buffer.strip()
