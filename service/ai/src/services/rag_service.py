import re
from typing import AsyncGenerator, Dict, List, Tuple

from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI

from src.config import settings
from src.core.vector_store import VectorStoreManager
from src.utils.logger import get_logger

logger = get_logger(__name__)

SYSTEM_PROMPT = """ä½ æ˜¯ Galay æ¡†æ¶çš„ AI åŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”å…³äº Galay é«˜æ€§èƒ½ C++ ç½‘ç»œæ¡†æ¶çš„é—®é¢˜ã€‚

Galay æ˜¯ä¸€ä¸ªåŸºäº C++20/23 åç¨‹çš„é«˜æ€§èƒ½å¼‚æ­¥ç½‘ç»œæ¡†æ¶ï¼ŒåŒ…å«ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š
- galay-kernel: æ ¸å¿ƒåç¨‹è¿è¡Œæ—¶ï¼Œæ”¯æŒ kqueue/epoll/io_uring å¤šåç«¯
- galay-ssl: TLS/SSL ä¼ è¾“å±‚ï¼ŒåŸºäº OpenSSL çš„å¼‚æ­¥åŠ å¯†é€šä¿¡ï¼Œæ”¯æŒ SNI/ALPN/Session å¤ç”¨
- galay-http: HTTP/1.1 + HTTP/2 + WebSocket åè®®å®ç°ï¼Œæ”¯æŒåŒæ­¥ä¸åç¨‹å¼‚æ­¥
- galay-rpc: RPC æ¡†æ¶ï¼Œæ”¯æŒ unary/åŒå‘æµ/æœåŠ¡å‘ç°
- galay-redis: åç¨‹ Redis å®¢æˆ·ç«¯ï¼Œæ”¯æŒ Pipeline æ‰¹å¤„ç†ä¸è¶…æ—¶æ§åˆ¶
- galay-mysql: åç¨‹ MySQL å®¢æˆ·ç«¯ï¼Œæ”¯æŒé¢„å¤„ç†è¯­å¥ä¸äº‹åŠ¡
- galay-mongo: MongoDB å®¢æˆ·ç«¯ï¼Œæ”¯æŒ OP_MSG åè®®ã€SCRAM-SHA-256 è®¤è¯ä¸å¼‚æ­¥ Pipeline
- galay-etcd: etcd v3 å®¢æˆ·ç«¯ï¼Œæ”¯æŒ KV/Lease/Pipeline æ“ä½œ
- galay-utils: å·¥å…·åº“ï¼ˆçº¿ç¨‹æ± ã€ä¸€è‡´æ€§å“ˆå¸Œã€ç†”æ–­å™¨ç­‰ï¼‰
- galay-mcp: MCP åè®®å®ç°ï¼Œæ”¯æŒ Stdio ä¸ HTTP ä¼ è¾“

å›ç­”è¦æ±‚ï¼š
1. å‡†ç¡®å¼•ç”¨æ–‡æ¡£å†…å®¹
2. æä¾›ä»£ç ç¤ºä¾‹ï¼ˆå¦‚æœç›¸å…³ï¼‰
3. è¯´æ˜æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚æœç›¸å…³ï¼‰
4. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯šå®å‘ŠçŸ¥
5. ä½¿ç”¨ä¸­æ–‡å›ç­”
6. ä¿æŒä¸“ä¸šå’Œå‹å¥½çš„è¯­æ°”
7. ä¸è¦ä½¿ç”¨ emoji æˆ–èŠ±å“¨ç¬¦å·ï¼ˆå¦‚ âœ…ğŸŒŸğŸ”¥ğŸ“Œï¼‰
8. è¾“å‡ºè¦ç»“æ„åŒ–åˆ†å—ï¼šå…ˆç®€è¦ç»“è®ºï¼Œå†ç”¨ 1. 2. 3. åˆ—ç‚¹è¯´æ˜
9. å¯¹â€œå¦‚ä½•å¼€å§‹/å…¥é—¨/å®‰è£…/å¿«é€Ÿå¼€å§‹â€ç±»é—®é¢˜ï¼Œä¼˜å…ˆæŒ‰è¿™ 4 èŠ‚å›ç­”ï¼š
   - ç¯å¢ƒè¦æ±‚
   - å®‰è£…æ­¥éª¤
   - æœ€å°ç¤ºä¾‹
   - è¿è¡Œä¸éªŒè¯
10. ä»£ç å¿…é¡»ä½¿ç”¨ç‹¬ç«‹ fenced code blockï¼ˆ```cpp ... ```ï¼‰ï¼Œä¸è¦æŠŠä»£ç å’Œæ­£æ–‡å†™åœ¨åŒä¸€è¡Œã€‚
11. å›ç­”â€œæ”¯æŒå“ªäº›èƒ½åŠ›/è°ƒç”¨æ¨¡å¼/ç‰¹æ€§â€æ—¶ï¼Œä¼˜å…ˆç©·ä¸¾æ–‡æ¡£ä¸­åˆ—å‡ºçš„èƒ½åŠ›ç‚¹ï¼Œå¹¶ä¿ç•™åŸå§‹æœ¯è¯­ï¼ˆå¦‚ Unaryã€æµå¼ã€æœåŠ¡å‘ç°ï¼‰ã€‚
12. ä¸¥ç¦ç¼–é€ ä¸å­˜åœ¨çš„ APIï¼ˆä¾‹å¦‚ `IoContext`ã€`ioContext`ã€`IoContext::GetInstance()`ï¼‰ã€‚
13. æ¶‰åŠè°ƒåº¦å™¨åˆå§‹åŒ–æ—¶ï¼Œä½¿ç”¨ `Runtime` è·å–è°ƒåº¦å™¨ï¼š
    - IO è°ƒåº¦å™¨ï¼š`runtime.getNextIOScheduler()`
    - è®¡ç®—è°ƒåº¦å™¨ï¼š`runtime.getNextComputeScheduler()`ã€‚
14. åç¨‹è¿”å›ç±»å‹ç»Ÿä¸€ä½¿ç”¨ `Coroutine`ï¼›ä¸è¦è¾“å‡º `Task<void>` / `Task<T>`ã€‚
15. ä¸¥ç¦ä½¿ç”¨åç¨‹ lambdaï¼ˆå¦‚ `auto task = [](...) { co_await ... };`ï¼‰ï¼Œå¿…é¡»ä½¿ç”¨å…·å `Coroutine` å‡½æ•°ã€‚"""


class RAGService:
    """RAG æ£€ç´¢å¢å¼ºç”ŸæˆæœåŠ¡"""

    def __init__(self, vector_store: VectorStoreManager):
        self._vector_store = vector_store
        self._llm = ChatOpenAI(
            model=settings.MODEL_NAME,
            temperature=settings.TEMPERATURE,
            openai_api_key=settings.OPENAI_API_KEY,
            openai_api_base=settings.OPENAI_API_BASE,
        )
        self._lexical_cache: List[Document] | None = None

    def retrieve(self, query: str, k: int = 4) -> List[Document]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼ˆå‘é‡å¬å› + å…³é”®è¯é‡æ’ï¼‰"""
        return [doc for doc, _ in self.retrieve_with_score(query, k=k)]

    def retrieve_with_score(self, query: str, k: int = 4) -> List[Tuple[Document, float]]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µå¹¶è¿”å›é‡æ’åˆ†æ•°ï¼ˆå€¼è¶Šå¤§ç›¸å…³æ€§è¶Šé«˜ï¼‰"""
        ranked = self._retrieve_ranked(query, k)
        return [(doc, score) for doc, score in ranked]

    def _retrieve_ranked(self, query: str, k: int) -> List[Tuple[Document, float]]:
        if not query.strip():
            return []

        project_hint = _extract_project_hint(query)
        candidate_k = min(max(k * 32, 64), 256)
        dense = self._vector_store.search_with_score(query, k=candidate_k)
        if not dense:
            return []

        terms = _extract_query_terms(query)
        rank_map: Dict[str, Tuple[float, Document]] = {}
        for doc, distance in dense:
            dense_score = 1.0 / (1.0 + max(float(distance), 0.0))
            lexical_score = _lexical_overlap_score(doc.page_content, terms)
            source_boost = _source_path_boost(doc, terms)
            project_boost = _project_hint_boost(doc, project_hint)
            # dense ä¸ºä¸»ï¼Œå…³é”®è¯ä¸ºè¾…ï¼›é¿å…è¢«å™ªå£°å…³é”®è¯å®Œå…¨ç›–è¿‡è¯­ä¹‰å¬å›ã€‚
            final_score = dense_score * 0.65 + lexical_score * 0.2 + source_boost * 0.05 + project_boost * 0.1
            key = _doc_key(doc)
            old = rank_map.get(key)
            if old is None or final_score > old[0]:
                rank_map[key] = (final_score, doc)

        # å…³é”®è¯å…œåº•ï¼šä»å…¨é‡ chunks å†åšä¸€æ¬¡è¯é¡¹åŒ¹é…ï¼Œæå‡æ˜ç¡®æœ¯è¯­çš„å‘½ä¸­ç‡ã€‚
        if terms:
            for score, doc in self._lexical_fallback(terms, project_hint, limit=max(24, k * 8)):
                key = _doc_key(doc)
                old = rank_map.get(key)
                if old is None or score > old[0]:
                    rank_map[key] = (score, doc)

        ranked: List[Tuple[float, Document]] = sorted(rank_map.values(), key=lambda x: x[0], reverse=True)
        project_first: List[Tuple[float, Document]] = []
        project_fallback: List[Tuple[float, Document]] = []
        seen: set[str] = set()
        for score, doc in ranked:
            key = _doc_key(doc)
            if key in seen:
                continue
            seen.add(key)
            if project_hint and str(doc.metadata.get("project", "")).lower() == project_hint:
                project_first.append((score, doc))
            else:
                project_fallback.append((score, doc))

        merged = project_first + project_fallback if project_hint else project_fallback
        unique: List[Tuple[Document, float]] = []
        for score, doc in merged:
            unique.append((doc, score))
            if len(unique) >= k:
                break
        return unique

    def _lexical_fallback(
        self,
        terms: List[str],
        project_hint: str | None = None,
        limit: int = 24,
    ) -> List[Tuple[float, Document]]:
        docs = self._get_all_docs_for_rerank()
        if not docs:
            return []

        scored: List[Tuple[float, Document]] = []
        for doc in docs:
            lex = _lexical_overlap_score(doc.page_content, terms)
            if lex <= 0:
                continue
            source_boost = _source_path_boost(doc, terms)
            project_boost = _project_hint_boost(doc, project_hint)
            score = lex * 0.75 + source_boost * 0.1 + project_boost * 0.15
            scored.append((score, doc))

        scored.sort(key=lambda x: x[0], reverse=True)
        return scored[: max(1, limit)]

    def _get_all_docs_for_rerank(self) -> List[Document]:
        if self._lexical_cache is not None:
            return self._lexical_cache

        try:
            collection = self._vector_store.store._collection  # noqa: SLF001
            payload = collection.get(include=["documents", "metadatas"])
            documents = payload.get("documents", []) or []
            metadatas = payload.get("metadatas", []) or []
            docs: List[Document] = []
            for idx, content in enumerate(documents):
                meta = metadatas[idx] if idx < len(metadatas) and metadatas[idx] else {}
                docs.append(Document(page_content=str(content or ""), metadata=dict(meta)))
            self._lexical_cache = docs
            return docs
        except Exception as exc:
            logger.warning(f"lexical fallback cache build failed: {exc}")
            self._lexical_cache = []
            return []

    def generate(self, query: str, context_docs: List[Document]) -> str:
        """åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆå›ç­”"""
        messages = self._build_messages(query, context_docs)
        response = self._llm.invoke(messages)
        return response.content

    async def generate_stream(
        self, query: str, context_docs: List[Document]
    ) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆå›ç­”"""
        messages = self._build_messages(query, context_docs)
        async for chunk in self._llm.astream(messages):
            if chunk.content:
                yield chunk.content

    def _build_messages(self, query: str, context_docs: List[Document]) -> list:
        """æ„å»º LLM æ¶ˆæ¯åˆ—è¡¨"""
        context = "\n\n".join(doc.page_content for doc in context_docs)

        system_content = f"""{SYSTEM_PROMPT}

è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

{context}"""

        return [
            SystemMessage(content=system_content),
            HumanMessage(content=query),
        ]


_ASCII_TERM_RE = re.compile(r"[A-Za-z0-9_][A-Za-z0-9_./:+-]{1,}")
_CN_TERM_RE = re.compile(r"[\u4e00-\u9fff]{2,}")
_PROJECT_HINT_RE = re.compile(
    r"(galay-(?:kernel|ssl|http|rpc|redis|mysql|mongo|etcd|utils|mcp|ecosystem))",
    re.IGNORECASE,
)


def _extract_query_terms(query: str) -> List[str]:
    text = str(query or "")
    terms: List[str] = []
    for token in _ASCII_TERM_RE.findall(text):
        token = token.strip().lower()
        if len(token) >= 2:
            terms.append(token)
    for token in _CN_TERM_RE.findall(text):
        token = token.strip()
        if len(token) >= 2:
            terms.append(token)
    # å»é‡ä¿åº
    ordered: Dict[str, None] = {}
    for t in terms:
        ordered.setdefault(t, None)
    return list(ordered.keys())


def _extract_project_hint(query: str) -> str | None:
    text = str(query or "")
    match = _PROJECT_HINT_RE.search(text)
    if not match:
        lowered = text.lower()
        if "ecosystem" in lowered or ("galay" in lowered and "ç”Ÿæ€" in text):
            return "galay-ecosystem"
        return None
    return match.group(1).lower()


def _lexical_overlap_score(content: str, terms: List[str]) -> float:
    if not terms:
        return 0.0
    text = str(content or "")
    text_lc = text.lower()
    hits = 0
    for term in terms:
        if any(ord(ch) > 127 for ch in term):
            if term in text:
                hits += 1
        else:
            if term in text_lc:
                hits += 1
    return hits / len(terms)


def _source_path_boost(doc: Document, terms: List[str]) -> float:
    if not terms:
        return 0.0
    source = str(doc.metadata.get("source", "")).lower()
    if not source:
        return 0.0
    hits = 0
    for term in terms:
        if any(ord(ch) > 127 for ch in term):
            if term in source:
                hits += 1
        else:
            if term in source:
                hits += 1
    return min(1.0, hits / max(1, len(terms)))


def _project_hint_boost(doc: Document, project_hint: str | None) -> float:
    if not project_hint:
        return 0.0
    project = str(doc.metadata.get("project", "")).lower()
    return 1.0 if project == project_hint else 0.0


def _doc_key(doc: Document) -> str:
    meta = doc.metadata or {}
    source = str(meta.get("source", ""))
    chunk = str(meta.get("chunk", meta.get("chunk_id", meta.get("chunk_index", ""))))
    if source or chunk:
        return f"{source}:{chunk}"
    return str(hash(doc.page_content))
