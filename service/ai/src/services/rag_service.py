import re
from typing import AsyncGenerator, Dict, List, Tuple

from langchain_core.documents import Document
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI

from src.config import settings
from src.core.vector_store import VectorStoreManager
from src.utils.logger import get_logger

logger = get_logger(__name__)

SYSTEM_PROMPT = """ä½ æ˜¯ Galay æ¡†æ¶çš„ AI åŠ©æ‰‹ï¼Œä¸“é—¨å›ç­”å…³äº Galay é«˜æ€§èƒ½ C++ ç½‘ç»œæ¡†æ¶çš„é—®é¢˜ã€‚

Galay æ˜¯ä¸€ä¸ªåŸºäº C++20/23 åç¨‹çš„é«˜æ€§èƒ½å¼‚æ­¥ç½‘ç»œæ¡†æ¶ï¼ŒåŒ…å«ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š
- galay-kernel: æ ¸å¿ƒåç¨‹è¿è¡Œæ—¶ï¼Œæ”¯æŒ kqueue/epoll/io_uring å¤šåç«¯
- galay-ssl: TLS/SSL ä¼ è¾“å±‚ï¼ŒåŸºäº OpenSSL çš„å¼‚æ­¥åŠ å¯†é€šä¿¡ï¼Œæ”¯æŒ SNI/ALPN/Session å¤ç”¨
- galay-http: HTTP/1.1 + HTTP/2 + WebSocket åè®®å®ç°ï¼Œæ”¯æŒåŒæ­¥ä¸åç¨‹å¼‚æ­¥
- galay-rpc: RPC æ¡†æ¶ï¼Œæ”¯æŒ unary/åŒå‘æµ/æœåŠ¡å‘ç°
- galay-redis: åç¨‹ Redis å®¢æˆ·ç«¯ï¼Œæ”¯æŒ Pipeline æ‰¹å¤„ç†ä¸è¶…æ—¶æ§åˆ¶
- galay-mysql: åç¨‹ MySQL å®¢æˆ·ç«¯ï¼Œæ”¯æŒé¢„å¤„ç†è¯­å¥ä¸äº‹åŠ¡
- galay-mongo: MongoDB å®¢æˆ·ç«¯ï¼Œæ”¯æŒ OP_MSG åè®®ã€SCRAM-SHA-256 è®¤è¯ä¸å¼‚æ­¥ Pipeline
- galay-etcd: etcd v3 å®¢æˆ·ç«¯ï¼Œæ”¯æŒ KV/Lease/Pipeline æ“ä½œ
- galay-utils: å·¥å…·åº“ï¼ˆçº¿ç¨‹æ± ã€ä¸€è‡´æ€§å“ˆå¸Œã€ç†”æ–­å™¨ç­‰ï¼‰
- galay-mcp: MCP åè®®å®ç°ï¼Œæ”¯æŒ Stdio ä¸ HTTP ä¼ è¾“

å›ç­”è¦æ±‚ï¼š
1. å‡†ç¡®å¼•ç”¨æ–‡æ¡£å†…å®¹
2. æä¾›ä»£ç ç¤ºä¾‹ï¼ˆå¦‚æœç›¸å…³ï¼‰
3. è¯´æ˜æ€§èƒ½æŒ‡æ ‡ï¼ˆå¦‚æœç›¸å…³ï¼‰
4. å¦‚æœæ–‡æ¡£ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯šå®å‘ŠçŸ¥
5. ä½¿ç”¨ä¸­æ–‡å›ç­”
6. ä¿æŒä¸“ä¸šå’Œå‹å¥½çš„è¯­æ°”
7. ä¸è¦ä½¿ç”¨ emoji æˆ–èŠ±å“¨ç¬¦å·ï¼ˆå¦‚ âœ…ğŸŒŸğŸ”¥ğŸ“Œï¼‰
8. è¾“å‡ºè¦ç»“æ„åŒ–åˆ†å—ï¼šå…ˆç®€è¦ç»“è®ºï¼Œå†ç”¨ 1. 2. 3. åˆ—ç‚¹è¯´æ˜
9. å¯¹â€œå¦‚ä½•å¼€å§‹/å…¥é—¨/å®‰è£…/å¿«é€Ÿå¼€å§‹â€ç±»é—®é¢˜ï¼Œä¼˜å…ˆæŒ‰è¿™ 4 èŠ‚å›ç­”ï¼š
   - ç¯å¢ƒè¦æ±‚
   - å®‰è£…æ­¥éª¤
   - æœ€å°ç¤ºä¾‹
   - è¿è¡Œä¸éªŒè¯
10. ä»£ç å¿…é¡»ä½¿ç”¨ç‹¬ç«‹ fenced code blockï¼ˆ```cpp ... ```ï¼‰ï¼Œä¸è¦æŠŠä»£ç å’Œæ­£æ–‡å†™åœ¨åŒä¸€è¡Œã€‚
11. å›ç­”â€œæ”¯æŒå“ªäº›èƒ½åŠ›/è°ƒç”¨æ¨¡å¼/ç‰¹æ€§â€æ—¶ï¼Œä¼˜å…ˆç©·ä¸¾æ–‡æ¡£ä¸­åˆ—å‡ºçš„èƒ½åŠ›ç‚¹ï¼Œå¹¶ä¿ç•™åŸå§‹æœ¯è¯­ï¼ˆå¦‚ Unaryã€æµå¼ã€æœåŠ¡å‘ç°ï¼‰ã€‚
12. ä¸¥ç¦ç¼–é€ ä¸å­˜åœ¨çš„ APIï¼ˆä¾‹å¦‚ `IoContext`ã€`ioContext`ã€`IoContext::GetInstance()`ï¼‰ã€‚
13. ä»…åœ¨è®²è§£ `galay-kernel` æˆ–æ˜¾å¼éœ€è¦æ‰‹åŠ¨è°ƒåº¦å™¨æ—¶ï¼Œæ‰ä½¿ç”¨ `Runtime` è·å–è°ƒåº¦å™¨ï¼š
    - IO è°ƒåº¦å™¨ï¼š`runtime.getNextIOScheduler()`
    - è®¡ç®—è°ƒåº¦å™¨ï¼š`runtime.getNextComputeScheduler()`ã€‚
14. æ¶‰åŠ `HttpServer` ç¤ºä¾‹æ—¶ï¼Œå¿…é¡»ä¸¥æ ¼ä½¿ç”¨æ–‡æ¡£æµç¨‹ï¼š
    - `HttpRouter router;`
    - `HttpServerConfig config;`
    - `HttpServer server(config);`
    - `server.start(std::move(router));`
    ä¸¥ç¦è¾“å‡ºæœªæ–‡æ¡£åŒ–å†™æ³•ï¼š`HttpServer server;`ã€`HttpServer(...scheduler...)`ã€`server.get/post/...`ã€`server.start(8080)`ã€‚
15. åç¨‹è¿”å›ç±»å‹ç»Ÿä¸€ä½¿ç”¨ `Coroutine`ï¼›ä¸è¦è¾“å‡º `Task<void>` / `Task<T>`ã€‚
16. ä¸¥ç¦ä½¿ç”¨åç¨‹ lambdaï¼ˆå¦‚ `auto task = [](...) { co_await ... };`ï¼‰ï¼Œå¿…é¡»ä½¿ç”¨å…·å `Coroutine` å‡½æ•°ã€‚
17. `Runtime` ä¸æ˜¯å•ä¾‹ï¼Œä¸å­˜åœ¨ `Runtime::getInstance()`ï¼Œåº”ç›´æ¥åˆ›å»ºå¯¹è±¡ï¼ˆå¦‚ `galay::kernel::Runtime runtime;`ï¼‰ã€‚
18. å›ç­” `galay-http` çš„å®‰è£…/æœ€å°ç¤ºä¾‹æ—¶ï¼Œä¾èµ–æ¸…å•å¿…é¡»åŒ…å« `galay-utils`ï¼ˆé€šå¸¸ä¸º `kernel + utils + http`ï¼ŒTLS åœºæ™¯å†åŠ  `ssl`ï¼‰ã€‚
19. åªè¦ç»™ C++ ä»£ç ç¤ºä¾‹ï¼Œå¿…é¡»åŒæ—¶ç»™ä¸¤æ®µä»£ç ï¼š`#include` ç‰ˆæœ¬ + `import` ç‰ˆæœ¬ï¼ˆå„è‡ªç‹¬ç«‹ fenced code blockï¼‰ã€‚
20. C++ ä»£ç æ’ç‰ˆå¿…é¡»ç»Ÿä¸€ï¼š4 ç©ºæ ¼ç¼©è¿›ï¼ŒèŠ±æ‹¬å·å¯¹é½ï¼Œ`#include` é¢„å¤„ç†è¡Œé¡¶æ ¼ã€‚
21. å›ç­”â€œç”¨æ³•/ç¤ºä¾‹/demo/example/å¿«é€Ÿå¼€å§‹â€æ—¶ï¼Œå¿…é¡»ä¼˜å…ˆä¾æ® demo/example/test/README/å¿«é€Ÿå¼€å§‹/ä½¿ç”¨ç¤ºä¾‹ä¸­å·²æœ‰è°ƒç”¨é“¾ï¼›è‹¥ç¤ºä¾‹æ²¡æœ‰è¯¥å†™æ³•ï¼Œå¿…é¡»æ˜ç¡®è¯´â€œç¤ºä¾‹ä¸­æœªæä¾›â€ï¼Œä¸è¦è¡¥è„‘ç¼–é€ ã€‚
22. ä»¥ä¸‹è°ƒç”¨é“¾å¿…é¡»ä¸ç¤ºä¾‹ä¿æŒä¸€è‡´ï¼š
    - `galay-http`ï¼š`HttpServerConfig` + `HttpRouter` + `HttpServer server(config)` + `server.start(std::move(router))`
    - `galay-rpc`ï¼š`RpcServerConfig` + `RpcServer server(config)` + `server.start()`
    - `galay-redis`ï¼š`Runtime` + `runtime.getNextIOScheduler()` + `RedisClient client(scheduler)`
    - `galay-mysql`ï¼š`AsyncMysqlClient client(scheduler)`ï¼ˆå¼‚æ­¥åœºæ™¯ï¼‰
    - `galay-mongo`ï¼š`AsyncMongoClient client(scheduler)`ï¼ˆå¼‚æ­¥åœºæ™¯ï¼‰
    - `galay-etcd`ï¼š`AsyncEtcdClient client(scheduler)`ï¼ˆå¼‚æ­¥åœºæ™¯ï¼‰
    - `galay-mcp`ï¼š`McpStdioServer::run()` æˆ– `McpHttpServer(host, port).start()`ã€‚"""

_USAGE_QUERY_RE = re.compile(
    r"(ç¤ºä¾‹|demo|example|sample|ç”¨æ³•|æ€ä¹ˆç”¨|å¦‚ä½•ç”¨|å¦‚ä½•ä½¿ç”¨|å…¥é—¨|å¿«é€Ÿå¼€å§‹|æœ€å°ç¤ºä¾‹|æœ€å°ç”¨ä¾‹|ä»£ç ç¤ºä¾‹|æ ·ä¾‹)",
    re.IGNORECASE,
)
_EXAMPLE_SOURCE_STRONG_HINTS = (
    "/example/",
    "/examples/",
    "/demo/",
    "/demos/",
    "/test/",
    "/tests/",
)
_EXAMPLE_SOURCE_WEAK_HINTS = (
    "readme.md",
    "å¿«é€Ÿå¼€å§‹",
    "ä½¿ç”¨ç¤ºä¾‹",
    "ç¤ºä¾‹ä»£ç ",
    "ä½¿ç”¨æŒ‡å—",
)


class RAGService:
    """RAG æ£€ç´¢å¢å¼ºç”ŸæˆæœåŠ¡"""

    def __init__(self, vector_store: VectorStoreManager):
        self._vector_store = vector_store
        self._llm = ChatOpenAI(
            model=settings.MODEL_NAME,
            temperature=settings.TEMPERATURE,
            openai_api_key=settings.OPENAI_API_KEY,
            openai_api_base=settings.OPENAI_API_BASE,
        )
        self._lexical_cache: List[Document] | None = None

    def retrieve(self, query: str, k: int = 4) -> List[Document]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼ˆå‘é‡å¬å› + å…³é”®è¯é‡æ’ï¼‰"""
        return [doc for doc, _ in self.retrieve_with_score(query, k=k)]

    def retrieve_with_score(self, query: str, k: int = 4) -> List[Tuple[Document, float]]:
        """æ£€ç´¢ç›¸å…³æ–‡æ¡£ç‰‡æ®µå¹¶è¿”å›é‡æ’åˆ†æ•°ï¼ˆå€¼è¶Šå¤§ç›¸å…³æ€§è¶Šé«˜ï¼‰"""
        ranked = self._retrieve_ranked(query, k)
        return [(doc, score) for doc, score in ranked]

    def _retrieve_ranked(self, query: str, k: int) -> List[Tuple[Document, float]]:
        if not query.strip():
            return []

        project_hint = _extract_project_hint(query)
        usage_intent = is_usage_query(query)
        candidate_k = min(max(k * 32, 64), 256)
        dense = self._vector_store.search_with_score(query, k=candidate_k)
        if not dense:
            return []

        terms = _extract_query_terms(query)
        rank_map: Dict[str, Tuple[float, Document]] = {}
        for doc, distance in dense:
            dense_score = 1.0 / (1.0 + max(float(distance), 0.0))
            lexical_score = _lexical_overlap_score(doc.page_content, terms)
            source_boost = _source_path_boost(doc, terms)
            project_boost = _project_hint_boost(doc, project_hint)
            example_boost = _example_source_boost(doc)
            # dense ä¸ºä¸»ï¼Œå…³é”®è¯ä¸ºè¾…ï¼›é¿å…è¢«å™ªå£°å…³é”®è¯å®Œå…¨ç›–è¿‡è¯­ä¹‰å¬å›ã€‚
            example_weight = 0.18 if usage_intent else 0.08
            final_score = (
                dense_score * 0.52
                + lexical_score * 0.16
                + source_boost * 0.04
                + project_boost * 0.1
                + example_boost * example_weight
            )
            key = _doc_key(doc)
            old = rank_map.get(key)
            if old is None or final_score > old[0]:
                rank_map[key] = (final_score, doc)

        # å…³é”®è¯å…œåº•ï¼šä»å…¨é‡ chunks å†åšä¸€æ¬¡è¯é¡¹åŒ¹é…ï¼Œæå‡æ˜ç¡®æœ¯è¯­çš„å‘½ä¸­ç‡ã€‚
        if terms:
            for score, doc in self._lexical_fallback(
                terms,
                project_hint,
                usage_intent=usage_intent,
                limit=max(24, k * 8),
            ):
                key = _doc_key(doc)
                old = rank_map.get(key)
                if old is None or score > old[0]:
                    rank_map[key] = (score, doc)

        ranked: List[Tuple[float, Document]] = sorted(rank_map.values(), key=lambda x: x[0], reverse=True)
        project_first: List[Tuple[float, Document]] = []
        project_fallback: List[Tuple[float, Document]] = []
        seen: set[str] = set()
        for score, doc in ranked:
            key = _doc_key(doc)
            if key in seen:
                continue
            seen.add(key)
            if project_hint and str(doc.metadata.get("project", "")).lower() == project_hint:
                project_first.append((score, doc))
            else:
                project_fallback.append((score, doc))

        merged = project_first + project_fallback if project_hint else project_fallback
        unique: List[Tuple[Document, float]] = []
        for score, doc in merged:
            unique.append((doc, score))
            if len(unique) >= k:
                break
        return unique

    def _lexical_fallback(
        self,
        terms: List[str],
        project_hint: str | None = None,
        usage_intent: bool = False,
        limit: int = 24,
    ) -> List[Tuple[float, Document]]:
        docs = self._get_all_docs_for_rerank()
        if not docs:
            return []

        scored: List[Tuple[float, Document]] = []
        for doc in docs:
            lex = _lexical_overlap_score(doc.page_content, terms)
            if lex <= 0:
                continue
            source_boost = _source_path_boost(doc, terms)
            project_boost = _project_hint_boost(doc, project_hint)
            example_boost = _example_source_boost(doc)
            example_weight = 0.15 if usage_intent else 0.05
            score = lex * 0.65 + source_boost * 0.08 + project_boost * 0.12 + example_boost * example_weight
            scored.append((score, doc))

        scored.sort(key=lambda x: x[0], reverse=True)
        return scored[: max(1, limit)]

    def _get_all_docs_for_rerank(self) -> List[Document]:
        if self._lexical_cache is not None:
            return self._lexical_cache

        try:
            collection = self._vector_store.store._collection  # noqa: SLF001
            payload = collection.get(include=["documents", "metadatas"])
            documents = payload.get("documents", []) or []
            metadatas = payload.get("metadatas", []) or []
            docs: List[Document] = []
            for idx, content in enumerate(documents):
                meta = metadatas[idx] if idx < len(metadatas) and metadatas[idx] else {}
                docs.append(Document(page_content=str(content or ""), metadata=dict(meta)))
            self._lexical_cache = docs
            return docs
        except Exception as exc:
            logger.warning(f"lexical fallback cache build failed: {exc}")
            self._lexical_cache = []
            return []

    def generate(self, query: str, context_docs: List[Document]) -> str:
        """åŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£ç”Ÿæˆå›ç­”"""
        messages = self._build_messages(query, context_docs)
        response = self._llm.invoke(messages)
        return response.content

    async def generate_stream(
        self, query: str, context_docs: List[Document]
    ) -> AsyncGenerator[str, None]:
        """æµå¼ç”Ÿæˆå›ç­”"""
        messages = self._build_messages(query, context_docs)
        async for chunk in self._llm.astream(messages):
            if chunk.content:
                yield chunk.content

    def _build_messages(self, query: str, context_docs: List[Document]) -> list:
        """æ„å»º LLM æ¶ˆæ¯åˆ—è¡¨"""
        context = format_context_docs(context_docs)
        guardrail = ""
        if is_usage_query(query) and not has_example_source(context_docs):
            guardrail = (
                "\nè¡¥å……çº¦æŸï¼šå½“å‰æ£€ç´¢ä¸Šä¸‹æ–‡æ²¡æœ‰å‘½ä¸­ demo/example/test/å¿«é€Ÿå¼€å§‹ç­‰ç¤ºä¾‹æ¥æºã€‚"
                "ä½ å¿…é¡»é¿å…ç¼–é€  APIï¼›è‹¥æ–‡æ¡£æœªç»™å‡ºå¯æ‰§è¡Œç¤ºä¾‹ï¼Œæ˜ç¡®è¯´æ˜â€œç¤ºä¾‹ä¸­æœªæä¾›è¯¥å†™æ³•â€ã€‚"
            )

        system_content = f"""{SYSTEM_PROMPT}
{guardrail}

è¯·åŸºäºä»¥ä¸‹æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ï¼š

{context}"""

        return [
            SystemMessage(content=system_content),
            HumanMessage(content=query),
        ]


_ASCII_TERM_RE = re.compile(r"[A-Za-z0-9_][A-Za-z0-9_./:+-]{1,}")
_CN_TERM_RE = re.compile(r"[\u4e00-\u9fff]{2,}")
_PROJECT_HINT_RE = re.compile(
    r"(galay-(?:kernel|ssl|http|rpc|redis|mysql|mongo|etcd|utils|mcp|ecosystem))",
    re.IGNORECASE,
)


def _extract_query_terms(query: str) -> List[str]:
    text = str(query or "")
    terms: List[str] = []
    for token in _ASCII_TERM_RE.findall(text):
        token = token.strip().lower()
        if len(token) >= 2:
            terms.append(token)
    for token in _CN_TERM_RE.findall(text):
        token = token.strip()
        if len(token) >= 2:
            terms.append(token)
    # å»é‡ä¿åº
    ordered: Dict[str, None] = {}
    for t in terms:
        ordered.setdefault(t, None)
    return list(ordered.keys())


def _extract_project_hint(query: str) -> str | None:
    text = str(query or "")
    match = _PROJECT_HINT_RE.search(text)
    if not match:
        lowered = text.lower()
        if "ecosystem" in lowered or ("galay" in lowered and "ç”Ÿæ€" in text):
            return "galay-ecosystem"
        return None
    return match.group(1).lower()


def is_usage_query(text: str) -> bool:
    return bool(_USAGE_QUERY_RE.search(str(text or "")))


def is_example_source_path(source: str) -> bool:
    normalized = str(source or "").replace("\\", "/").lower()
    if not normalized:
        return False
    if any(hint in normalized for hint in _EXAMPLE_SOURCE_STRONG_HINTS):
        return True
    if any(hint in normalized for hint in _EXAMPLE_SOURCE_WEAK_HINTS):
        return True
    return False


def has_example_source(docs: List[Document]) -> bool:
    for doc in docs:
        source = str(doc.metadata.get("source", ""))
        if is_example_source_path(source):
            return True
    return False


def format_context_docs(context_docs: List[Document]) -> str:
    sections: List[str] = []
    for index, doc in enumerate(context_docs, start=1):
        project = str(doc.metadata.get("project", "unknown"))
        source = str(doc.metadata.get("source", "unknown"))
        header = f"[{index}] project={project} source={source}"
        body = str(doc.page_content or "").strip()
        if not body:
            continue
        sections.append(f"{header}\n{body}")
    return "\n\n".join(sections)


def _lexical_overlap_score(content: str, terms: List[str]) -> float:
    if not terms:
        return 0.0
    text = str(content or "")
    text_lc = text.lower()
    hits = 0
    for term in terms:
        if any(ord(ch) > 127 for ch in term):
            if term in text:
                hits += 1
        else:
            if term in text_lc:
                hits += 1
    return hits / len(terms)


def _source_path_boost(doc: Document, terms: List[str]) -> float:
    if not terms:
        return 0.0
    source = str(doc.metadata.get("source", "")).lower()
    if not source:
        return 0.0
    hits = 0
    for term in terms:
        if any(ord(ch) > 127 for ch in term):
            if term in source:
                hits += 1
        else:
            if term in source:
                hits += 1
    return min(1.0, hits / max(1, len(terms)))


def _example_source_boost(doc: Document) -> float:
    source = str(doc.metadata.get("source", ""))
    normalized = source.replace("\\", "/").lower()
    if not normalized:
        return 0.0
    if any(hint in normalized for hint in _EXAMPLE_SOURCE_STRONG_HINTS):
        return 1.0
    if any(hint in normalized for hint in _EXAMPLE_SOURCE_WEAK_HINTS):
        return 0.65
    return 0.0


def _project_hint_boost(doc: Document, project_hint: str | None) -> float:
    if not project_hint:
        return 0.0
    project = str(doc.metadata.get("project", "")).lower()
    return 1.0 if project == project_hint else 0.0


def _doc_key(doc: Document) -> str:
    meta = doc.metadata or {}
    source = str(meta.get("source", ""))
    chunk = str(meta.get("chunk", meta.get("chunk_id", meta.get("chunk_index", ""))))
    if source or chunk:
        return f"{source}:{chunk}"
    return str(hash(doc.page_content))
